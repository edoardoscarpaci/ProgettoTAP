{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from elasticsearch import Elasticsearch\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbbc67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'HOSTNAME': '2fbfb92b6cc1',\n",
       "        'JAVA_HOME': '/usr/local/openjdk-8',\n",
       "        'PWD': '/opt/spark-2.4.8-bin-hadoop2.7',\n",
       "        'HOME': '/root',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'SPARK_ACTION': 'pyspark',\n",
       "        'PYSPARK_PYTHON': '/usr/bin/python3',\n",
       "        'PYTHONPATH': '/opt/spark/python/:/opt/spark/python/lib/py4j-0.10.7-src.zip:/opt/spark/python/lib/pyspark.zip',\n",
       "        'SPARK_DIR': '/opt/spark',\n",
       "        'HADOOP_VERSION': '2.7',\n",
       "        'SPARK_VERSION': '2.4.8',\n",
       "        'SHLVL': '1',\n",
       "        'SPARK_HOME': '/opt/spark',\n",
       "        'PATH': '/opt/spark/bin:/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'JAVA_VERSION': '8u332',\n",
       "        '_': '/usr/local/bin/jupyter',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_PARENT_PID': '32',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5ec321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apt\t     dpkg   gnupg2   mime\t ssl\t     udev\r\n",
      "bfd-plugins  gcc    gold-ld  os-release  systemd     valgrind\r\n",
      "compat-ld    gnupg  locale   sasl2\t tmpfiles.d  x86_64-linux-gnu\r\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d518c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/opt/spark-2.4.8-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency\n",
      "org.elasticsearch#elasticsearch-spark-20_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8637c5ec-4819-4e57-b141-d2e50e6c5734;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.7 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.0.0 in central\n",
      "\tfound org.lz4#lz4-java;1.4.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.elasticsearch#elasticsearch-spark-20_2.11;7.17.4 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.11;2.4.4 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.4.7/spark-sql-kafka-0-10_2.11-2.4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.7!spark-sql-kafka-0-10_2.11.jar (658ms)\n",
      "downloading https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.11/7.17.4/elasticsearch-spark-20_2.11-7.17.4.jar ...\n",
      "\t[SUCCESSFUL ] org.elasticsearch#elasticsearch-spark-20_2.11;7.17.4!elasticsearch-spark-20_2.11.jar (554ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.0.0!kafka-clients.jar (253ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (35ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.4.0!lz4-java.jar (69ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (211ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (40ms)\n",
      ":: resolution report :: resolve 16080ms :: artifacts dl 1835ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.11;2.4.7 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.11;2.4.4 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-20_2.11;7.17.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.4.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.6 by [org.slf4j#slf4j-api;1.7.16] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   12  |   12  |   1   ||   7   |   7   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8637c5ec-4819-4e57-b141-d2e50e6c5734\n",
      "\tconfs: [default]\n",
      "\t7 artifacts copied, 0 already retrieved (6790kB/26ms)\n",
      "22/06/16 11:30:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\") \\\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a9c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n"
     ]
    }
   ],
   "source": [
    "es_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"Nome\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"vector\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 200\n",
    "            },\n",
    "            \"pca\": {\n",
    "                \"type\": \"point\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://10.0.100.51:9200\") \n",
    "response = es.indices.create(\n",
    "    index=\"vectorsteps\",\n",
    "    body=es_mapping,\n",
    "    ignore=400 # ignore 400 already exists code\n",
    ")\n",
    "if 'acknowledged' in response:\n",
    "    if response['acknowledged'] == True:\n",
    "        print (\"INDEX MAPPING SUCCESS FOR INDEX:\", response['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a799d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"Link\",StringType(),True), \\\n",
    "    StructField(\"Steps\",StringType(),True), \\\n",
    "    StructField(\"Ingredienti\",ArrayType(StructType([\n",
    "         StructField('Nome', StringType(), True),\n",
    "         StructField('Peso', StringType(), True),\n",
    "         StructField('Note', StringType(), True)\n",
    "         ])),True), \\\n",
    "    StructField(\"Nome\", StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = spark.read\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"subscribe\", \"CleanedRecipes\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"10.0.100.23:9092\")\\\n",
    "  .option(\"kafka.request.timeout.ms\", \"60000\")\\\n",
    "  .option(\"kafka.session.timeout.ms\", \"60000\")\\\n",
    "  .option(\"failOnDataLoss\", \"true\")\\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"partition\", 1) \\\n",
    "  .option(\"fetch.message.max.bytes\",\"52428800\")\\\n",
    "  .option(\"kafka.max.poll.records\", \"1000\")\\\n",
    "  .load()\\\n",
    "  .limit(1000)\\\n",
    "\n",
    "tr = tr.selectExpr(\"CAST(value AS STRING)\")\n",
    "tr = tr.select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")    \n",
    "tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4800d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_1 = RegexTokenizer(inputCol= 'Steps' , outputCol= 'steptokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'steptokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 200)\n",
    "# setup the pipeline\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"vector\")\n",
    "pca.setOutputCol(\"pca\")\n",
    "\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3,pca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pipelineFit.transform(tr).select(\"Nome\",\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"subscribe\", \"CleanedRecipes\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"10.0.100.23:9092\")\\\n",
    "  .option(\"kafka.request.timeout.ms\", \"60000\")\\\n",
    "  .option(\"kafka.session.timeout.ms\", \"60000\")\\\n",
    "  .option(\"failOnDataLoss\", \"true\")\\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"partition\", 1) \\\n",
    "  .option(\"fetch.message.max.bytes\",\"52428800\")\\\n",
    "  .load()\\\n",
    "  .selectExpr(\"CAST(value AS STRING)\")\\\n",
    "  .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")    \n",
    "\n",
    "df = pipelineFit.transform(df).select(\"Nome\",\"vector\",\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "\n",
    "to_array = F.udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "df = df.withColumn('pca', to_array('pca'))\n",
    "df = df.withColumn('vector', to_array('vector'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "259aaa33",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o678.start.\n: java.lang.IllegalStateException: Cannot start query with id 66f6d642-69cc-49af-999f-b20351c24595 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:345)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:326)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/2340672692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.nodes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"10.0.100.51\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/save/location\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vectorsteps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o678.start.\n: java.lang.IllegalStateException: Cannot start query with id 66f6d642-69cc-49af-999f-b20351c24595 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:345)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:326)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"org.elasticsearch.spark.sql\")\\\n",
    "    .option(\"es.nodes\",\"10.0.100.51\")\\\n",
    "    .option(\"checkpointLocation\", \"/save/location\") \\\n",
    "    .start(\"vectorsteps\") \\\n",
    "    .awaitTermination()\n",
    " \n",
    "spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79016a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
