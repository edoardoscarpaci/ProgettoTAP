{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from elasticsearch import Elasticsearch\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbbc67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'HOSTNAME': '2fbfb92b6cc1',\n",
       "        'JAVA_HOME': '/usr/local/openjdk-8',\n",
       "        'PWD': '/opt/spark-2.4.8-bin-hadoop2.7',\n",
       "        'HOME': '/root',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'SPARK_ACTION': 'pyspark',\n",
       "        'PYSPARK_PYTHON': '/usr/bin/python3',\n",
       "        'PYTHONPATH': '/opt/spark/python/:/opt/spark/python/lib/py4j-0.10.7-src.zip:/opt/spark/python/lib/pyspark.zip',\n",
       "        'SPARK_DIR': '/opt/spark',\n",
       "        'HADOOP_VERSION': '2.7',\n",
       "        'SPARK_VERSION': '2.4.8',\n",
       "        'SHLVL': '1',\n",
       "        'SPARK_HOME': '/opt/spark',\n",
       "        'PATH': '/opt/spark/bin:/usr/local/openjdk-8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'JAVA_VERSION': '8u332',\n",
       "        '_': '/usr/local/bin/jupyter',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_PARENT_PID': '32',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5ec321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apt\t     dpkg   gnupg2   mime\t ssl\t     udev\r\n",
      "bfd-plugins  gcc    gold-ld  os-release  systemd     valgrind\r\n",
      "compat-ld    gnupg  locale   sasl2\t tmpfiles.d  x86_64-linux-gnu\r\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d518c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/opt/spark-2.4.8-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency\n",
      "org.elasticsearch#elasticsearch-spark-20_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8637c5ec-4819-4e57-b141-d2e50e6c5734;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.7 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.0.0 in central\n",
      "\tfound org.lz4#lz4-java;1.4.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.elasticsearch#elasticsearch-spark-20_2.11;7.17.4 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.11;2.4.4 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.4.7/spark-sql-kafka-0-10_2.11-2.4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.7!spark-sql-kafka-0-10_2.11.jar (658ms)\n",
      "downloading https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-20_2.11/7.17.4/elasticsearch-spark-20_2.11-7.17.4.jar ...\n",
      "\t[SUCCESSFUL ] org.elasticsearch#elasticsearch-spark-20_2.11;7.17.4!elasticsearch-spark-20_2.11.jar (554ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.0.0!kafka-clients.jar (253ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (35ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.4.0!lz4-java.jar (69ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (211ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (40ms)\n",
      ":: resolution report :: resolve 16080ms :: artifacts dl 1835ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.11;2.4.7 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.11;2.4.4 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-20_2.11;7.17.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.4.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.6 by [org.slf4j#slf4j-api;1.7.16] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   12  |   12  |   1   ||   7   |   7   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8637c5ec-4819-4e57-b141-d2e50e6c5734\n",
      "\tconfs: [default]\n",
      "\t7 artifacts copied, 0 already retrieved (6790kB/26ms)\n",
      "22/06/16 11:30:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\") \\\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a9c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:28: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n"
     ]
    }
   ],
   "source": [
    "es_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"Nome\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"vector\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 200\n",
    "            },\n",
    "            \"pca\": {\n",
    "                \"type\": \"point\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es = Elasticsearch(hosts=\"http://10.0.100.51:9200\") \n",
    "response = es.indices.create(\n",
    "    index=\"vectorsteps\",\n",
    "    body=es_mapping,\n",
    "    ignore=400 # ignore 400 already exists code\n",
    ")\n",
    "if 'acknowledged' in response:\n",
    "    if response['acknowledged'] == True:\n",
    "        print (\"INDEX MAPPING SUCCESS FOR INDEX:\", response['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a799d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"Link\",StringType(),True), \\\n",
    "    StructField(\"Steps\",StringType(),True), \\\n",
    "    StructField(\"Ingredienti\",ArrayType(StructType([\n",
    "         StructField('Nome', StringType(), True),\n",
    "         StructField('Peso', StringType(), True),\n",
    "         StructField('Note', StringType(), True)\n",
    "         ])),True), \\\n",
    "    StructField(\"Nome\", StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1a77e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Link: string, Steps: string, Ingredienti: array<struct<Nome:string,Peso:string,Note:string>>, Nome: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = spark.read\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"subscribe\", \"CleanedRecipes\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"10.0.100.23:9092\")\\\n",
    "  .option(\"kafka.request.timeout.ms\", \"60000\")\\\n",
    "  .option(\"kafka.session.timeout.ms\", \"60000\")\\\n",
    "  .option(\"failOnDataLoss\", \"true\")\\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"partition\", 1) \\\n",
    "  .option(\"fetch.message.max.bytes\",\"52428800\")\\\n",
    "  .option(\"kafka.max.poll.records\", \"1000\")\\\n",
    "  .load()\\\n",
    "  .limit(1000)\\\n",
    "\n",
    "tr = tr.selectExpr(\"CAST(value AS STRING)\")\n",
    "tr = tr.select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")    \n",
    "tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b26c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d4800d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_1 = RegexTokenizer(inputCol= 'Steps' , outputCol= 'steptokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'steptokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 200)\n",
    "# setup the pipeline\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"vector\")\n",
    "pca.setOutputCol(\"pca\")\n",
    "\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3,pca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26d1b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pipelineFit.transform(tr).select(\"Nome\",\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce3eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                Nome|                 pca|\n",
      "+--------------------+--------------------+\n",
      "|            Tiramisù|[-0.1007319794162...|\n",
      "|Pancake allo scir...|[-0.0292037807742...|\n",
      "|Spaghetti alla Ca...|[0.11258745261459...|\n",
      "|Crepe dolci e sal...|[-0.1155195015620...|\n",
      "|  Pasta per la pizza|[-0.1334991362682...|\n",
      "| New York Cheesecake|[-0.0244016179456...|\n",
      "|Spaghetti all'Ama...|[0.18186021881662...|\n",
      "|      Torta tenerina|[-0.0751187076385...|\n",
      "|Tortino di ciocco...|[-0.0554144511031...|\n",
      "|            Brownies|[7.25502825981291...|\n",
      "|        Banana bread|[-0.0027489154839...|\n",
      "|         Besciamella|[0.00352168886721...|\n",
      "|Spaghetti alle vo...|[0.14287876918852...|\n",
      "|Spaghetti Cacio e...|[0.04638765563924...|\n",
      "|Parmigiana di mel...|[0.10785743562569...|\n",
      "|Muffin con gocce ...|[-0.0610205523530...|\n",
      "|Lasagne alla Bolo...|[-0.0041416940590...|\n",
      "|Crostata di frago...|[-0.0190113538053...|\n",
      "|       Torta di mele|[0.03998592626912...|\n",
      "|        Chiffon cake|[-0.0929102645726...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c824670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"subscribe\", \"CleanedRecipes\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"10.0.100.23:9092\")\\\n",
    "  .option(\"kafka.request.timeout.ms\", \"60000\")\\\n",
    "  .option(\"kafka.session.timeout.ms\", \"60000\")\\\n",
    "  .option(\"failOnDataLoss\", \"true\")\\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"partition\", 1) \\\n",
    "  .option(\"fetch.message.max.bytes\",\"52428800\")\\\n",
    "  .load()\\\n",
    "  .selectExpr(\"CAST(value AS STRING)\")\\\n",
    "  .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")    \n",
    "\n",
    "df = pipelineFit.transform(df).select(\"Nome\",\"vector\",\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3c16d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Nome: string, vector: array<float>, pca: array<float>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "\n",
    "to_array = F.udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "df = df.withColumn('pca', to_array('pca'))\n",
    "df = df.withColumn('vector', to_array('vector'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259aaa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"org.elasticsearch.spark.sql\")\\\n",
    "    .option(\"es.nodes\",\"10.0.100.51\")\\\n",
    "    .option(\"checkpointLocation\", \"/save/location\") \\\n",
    "    .start(\"vectorsteps\") \\\n",
    "    .awaitTermination()\n",
    " \n",
    "spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79016a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
